{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('base': conda)",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "09c573d9dec07aaa3b244ebe701bc7e75f0e320926908027be5a112cb68961b1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import timedelta\n",
    "\n",
    "'''\n",
    "Note: 2 way to index an element in pandas df\n",
    "By Index:\n",
    "- df.Time[row]\n",
    "- df.loc[row, 'Time']\n",
    "\n",
    "By location:\n",
    "- df.iloc[1, 1]: iloc only take interger\n",
    "\n",
    "'''\n",
    "def calculateTimeInterval(missing_data):\n",
    "    df = missing_data.copy()\n",
    "    df['TimeInterval'] = (df['CompleteTimestamp'] - df['CompleteTimestamp'].shift(1))\n",
    "    df.loc[0, 'TimeInterval'] = 0\n",
    "    return df\n",
    "\n",
    "def calculateDuration(missing_data):\n",
    "    df = missing_data.copy()\n",
    "    df['Duration'] = (df['CompleteTimestamp'] - df['CompleteTimestamp'].iloc[0])\n",
    "    df['Duration'].iloc[0] = 0\n",
    "    return df\n",
    "\n",
    "def calculateCumTimeInterval(missing_data):\n",
    "    df = missing_data.copy()\n",
    "    df['CumTimeInterval'] = (df['CompleteTimestamp'] - df['CompleteTimestamp'].iloc[0])\n",
    "    return df\n",
    "\n",
    "def calculateCaseTimeInterval(missing_data):\n",
    "    df = missing_data.copy()\n",
    "    df['CaseTimeInterval'] = np.nan\n",
    "\n",
    "    current_point = {}\n",
    "    current_point[df.loc[0, 'CaseID']] = df.loc[0, 'CompleteTimestamp']\n",
    "\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if df.loc[i, 'CaseID'] != df.loc[i-1, 'CaseID']:\n",
    "            df.loc[i, 'CaseTimeInterval'] = (df.loc[i, 'CompleteTimestamp'] - current_point[df.loc[i, 'CaseID']-1]).total_seconds()\n",
    "            current_point[df.loc[i, 'CaseID']] = df.loc[i, 'CompleteTimestamp']\n",
    "            \n",
    "    return df\n",
    "\n",
    "def convert2seconds(x):\n",
    "    x = x.total_seconds()\n",
    "    return x\n",
    "\n",
    "\n",
    "def minmaxScaler(caseid, df_case, missing_df_case):\n",
    "    epsilon = 0.1\n",
    "    missing_case_storage = {}\n",
    "    missing_case_storage[caseid] = {}\n",
    "    \n",
    "    temp = df_case.copy()\n",
    "    missing_temp = missing_df_case.copy()\n",
    "    \n",
    "    temp['NormalizedTime'] = temp['CumTimeInterval'].copy()\n",
    "    missing_temp['NormalizedTime'] = missing_temp['CumTimeInterval'].copy()\n",
    "    \n",
    "    min_val = temp['CumTimeInterval'].min()\n",
    "    max_val = temp['CumTimeInterval'].max()\n",
    "    \n",
    "    missing_min_val = missing_temp['CumTimeInterval'].min()\n",
    "    missing_max_val = missing_temp['CumTimeInterval'].max()\n",
    "    missing_case_storage[caseid]['missing_min_val'] = missing_min_val\n",
    "    missing_case_storage[caseid]['missing_max_val'] = missing_max_val\n",
    "    \n",
    "    for row in range(temp.shape[0]):\n",
    "        #scale complete df\n",
    "        temp.iloc[row, temp.columns.get_loc('NormalizedTime')] = (temp.iloc[row, temp.columns.get_loc('CumTimeInterval')] - min_val)/(max_val-min_val+epsilon)\n",
    "        \n",
    "        #scale missing df\n",
    "        missing_temp.iloc[row, missing_temp.columns.get_loc('NormalizedTime')] = (missing_temp.iloc[row, missing_temp.columns.get_loc('CumTimeInterval')] - missing_min_val)/(missing_max_val-missing_min_val+epsilon)  \n",
    "    return temp, missing_temp, missing_case_storage\n",
    "\n",
    "\n",
    "def OHE(df, categorical_variables):\n",
    "    for i in categorical_variables:\n",
    "        enc_df = pd.get_dummies(df, columns=categorical_variables, drop_first=False)\n",
    "    return enc_df\n",
    "\n",
    "def findLongestLength(groupByCase):\n",
    "    '''This function returns the length of longest case'''\n",
    "    #groupByCase = data.groupby(['CaseID'])\n",
    "    maxlen = 1\n",
    "    for case, group in groupByCase:\n",
    "        temp_len = group.shape[0]\n",
    "        if temp_len > maxlen:\n",
    "            maxlen = temp_len\n",
    "    return maxlen\n",
    "\n",
    "def padwithzeros(vector, maxlen):\n",
    "    '''This function returns the (maxlen, num_features) vector padded with zeros'''\n",
    "    npad = ((maxlen-vector.shape[0], 0), (0, 0))\n",
    "    padded_vector = np.pad(vector, pad_width=npad, mode='constant', constant_values=0)\n",
    "    return padded_vector\n",
    "\n",
    "def getInput(groupByCase, cols, maxlen):\n",
    "    full_list = []\n",
    "    for case, data in groupByCase:\n",
    "        temp = data.to_numpy()#data.as_matrix(columns=cols)\n",
    "        temp = padwithzeros(temp, maxlen)\n",
    "        full_list.append(temp)\n",
    "    inp = np.array(full_list)\n",
    "    return inp\n",
    "\n",
    "def getMeanVar(array, idx=0):\n",
    "    temp_array = [a[idx] for a in array if not np.isnan(a[idx])]\n",
    "    mean_val = np.mean(temp_array)\n",
    "    var_val = np.var(temp_array)\n",
    "    return mean_val, var_val\n",
    "\n",
    "def getProbability(recon_test):\n",
    "    '''This function takes 3d tensor as input and return a 3d tensor which has \n",
    "    probabilities for classes of categorical variable'''\n",
    "    softmax = nn.Softmax()\n",
    "    #recon_test = recon_test.cpu() #moving data from gpu to cpu for full evaluation\n",
    "    \n",
    "    for i in range(recon_test.size(0)):\n",
    "        cont_values = recon_test[i, :, 0].contiguous().view(recon_test.size(1),1) #(35,1)\n",
    "        #softmax_values = softmax(recon_test[i, :, 1:])\n",
    "        softmax_v = softmax()\n",
    "        softmax_values = softmax_v.fit(recon_test[i, :, 1:])\n",
    "        if i == 0:\n",
    "            recon = torch.cat([cont_values, softmax_values], 1)\n",
    "            recon = recon.contiguous().view(1,recon_test.size(1), recon_test.size(2)) #(1, 35, 8)\n",
    "        else:\n",
    "            current_recon = torch.cat([cont_values, softmax_values], 1)\n",
    "            current_recon = current_recon.contiguous().view(1,recon_test.size(1), recon_test.size(2)) #(1, 35, 8)\n",
    "            recon = torch.cat([recon, current_recon], 0)\n",
    "    return recon\n",
    "\n",
    "def convert2df(predicted_tensor, pad_matrix, cols, test_row_num):\n",
    "    '''\n",
    "    This function converts a tensor to a pandas dataframe\n",
    "    Return: Dataframe with columns (NormalizedTime, PredictedActivity)\n",
    "\n",
    "    - predicted_tensor: recon\n",
    "    - df: recon_df_w_normalized_time\n",
    "    '''\n",
    "    print(\"WORKS\")\n",
    "    #predicted_tensor = getProbability(predicted_tensor) #get probability for categorical variables\n",
    "    predicted_array = predicted_tensor.data.cpu().numpy() #convert to numpy array\n",
    "    \n",
    "    #Remove 0-padding\n",
    "    temp_array = np.array(predicted_array) * np.array(pad_matrix)\n",
    "    temp_array = temp_array.reshape(predicted_array.shape[0]*predicted_array.shape[1], predicted_array.shape[2])\n",
    "    temp_array = temp_array[np.any(temp_array != 0, axis=1)]\n",
    "    \n",
    "    #check number of row of df\n",
    "    if temp_array.shape[0] == test_row_num:\n",
    "        #print('Converting tensor to dataframe...')\n",
    "        df = pd.DataFrame(temp_array, columns=cols)\n",
    "        activity_list = [i for i in cols if i!='NormalizedTime']\n",
    "        df['PredictedActivity'] = df[activity_list].idxmax(axis=1) #get label\n",
    "        #df['PredictedActivity'] = df['PredictedActivity'].apply(lambda x: x.split('_')[1]) #remove prefix\n",
    "        df['PredictedActivity'] = df['PredictedActivity'].apply(lambda x: x[9:]) #remove prefix Activity_\n",
    "        df = df.drop(activity_list, axis=1)\n",
    "        #print('Done!!!')\n",
    "    return df\n",
    "\n",
    "def inversedMinMaxScaler(caseid, min_max_storage, recon_df_w_normalized_time_case):\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    temp = recon_df_w_normalized_time_case.copy()\n",
    "    temp['PredictedCumTimeInterval'] = recon_df_w_normalized_time_case['NormalizedTime'].copy()\n",
    "    \n",
    "    #should check for nan values here\n",
    "    #min_val = min_max_storage[caseid]['missing_min_val']\n",
    "    #max_val = min_max_storage[caseid]['missing_max_val']\n",
    "    min_val, max_val = findValidMinMax(caseid, min_max_storage)\n",
    "\n",
    "    for row in range(temp.shape[0]):\n",
    "        temp.iloc[row, temp.columns.get_loc('PredictedCumTimeInterval')] = min_val + temp.iloc[row, temp.columns.get_loc('NormalizedTime')]*(max_val-min_val+epsilon)\n",
    "        \n",
    "    return temp\n",
    "\n",
    "def findValidMinMax(caseid, min_max_storage):\n",
    "    min_val_before = 0\n",
    "    max_val_before= 0\n",
    "    min_val_after = 0\n",
    "    max_val_after = 0\n",
    "    min_val = 0\n",
    "    max_val = 0\n",
    "    \n",
    "    if caseid == len(min_max_storage):\n",
    "        for i in range(caseid):\n",
    "            min_val = min_max_storage[caseid-i]['missing_min_val']\n",
    "            max_val = min_max_storage[caseid-i]['missing_max_val']\n",
    "            if not np.isnan(min_val) and not np.isnan(max_val):\n",
    "                break\n",
    "    else:\n",
    "        for i in range(caseid):\n",
    "            min_val_before = min_max_storage[caseid-i]['missing_min_val']\n",
    "            max_val_before = min_max_storage[caseid-i]['missing_max_val']\n",
    "            if not np.isnan(min_val_before) and not np.isnan(max_val_before):\n",
    "                break\n",
    "    \n",
    "        for j in range(len(min_max_storage) - caseid+1):\n",
    "            min_val_after = min_max_storage[caseid+j]['missing_min_val']\n",
    "            max_val_after = min_max_storage[caseid+j]['missing_max_val']\n",
    "            if not np.isnan(min_val_after) and not np.isnan(max_val_after):\n",
    "                break\n",
    "        min_val = (min_val_before+min_val_after)/2\n",
    "        max_val = (max_val_before+max_val_after)/2\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "def getDfWithTime(recon_df_w_normalized_time, missing_true_test, min_max_storage):\n",
    "    temp = recon_df_w_normalized_time.copy()\n",
    "    temp['CaseID'] = missing_true_test['CaseID'].copy()\n",
    "    recon_groupByCase = temp.groupby(['CaseID'])\n",
    "    recon_df_w_time = pd.DataFrame(columns=list(temp)+['PredictedCumTimeInterval'])\n",
    "    \n",
    "    for caseid, data_case in recon_groupByCase:\n",
    "        temp_case = inversedMinMaxScaler(caseid, min_max_storage, data_case)\n",
    "        recon_df_w_time = recon_df_w_time.append(temp_case)\n",
    "    return recon_df_w_time\n",
    "\n",
    "\n",
    "\n",
    "def getnanindex(missing_true_df):\n",
    "    nan_time_index = []\n",
    "    nan_activity_index = []\n",
    "    for row in range(missing_true_df.shape[0]):\n",
    "        if np.isnan(missing_true_df.CumTimeInterval[row]):\n",
    "            nan_time_index.append(row)\n",
    "\n",
    "        if not type(missing_true_df.Activity[row]) == str:\n",
    "            nan_activity_index.append(row)\n",
    "    return nan_time_index, nan_activity_index\n",
    "\n",
    "def getSubmission(recon_df_w_time, missing_true_test, complete_true_test, first_timestamp):\n",
    "    temp = pd.DataFrame(columns=['CaseID', 'TrueActivity', 'PredictedActivity', 'TrueTime', 'PredictedTime'])\n",
    "    temp['CaseID'] = missing_true_test['CaseID'].copy()\n",
    "    \n",
    "    #ground truth\n",
    "    temp['TrueActivity'] = complete_true_test['Activity'].copy()\n",
    "    temp['TrueTime'] = complete_true_test['CumTimeInterval'].copy()\n",
    "    temp['TrueCompleteTimestamp'] = complete_true_test['CompleteTimestamp'].copy()\n",
    "\n",
    "    #predicted activity\n",
    "    temp['PredictedActivity'] = missing_true_test['Activity'].copy()\n",
    "    temp['PredictedTime'] = missing_true_test['CumTimeInterval'].copy()\n",
    "    temp['PredictedCompleteTimestamp'] = missing_true_test['CompleteTimestamp'].copy()\n",
    "\n",
    "    for row in range(temp.shape[0]):\n",
    "        if pd.isnull(temp.loc[row, 'PredictedActivity']):\n",
    "            temp.loc[row, 'PredictedActivity'] = recon_df_w_time.loc[row, 'PredictedActivity']\n",
    "        if pd.isnull(temp.loc[row, 'PredictedTime']):\n",
    "            temp.loc[row, 'PredictedTime'] = recon_df_w_time.loc[row, 'PredictedCumTimeInterval']\n",
    "            temp.loc[row, 'PredictedCompleteTimestamp'] = first_timestamp+timedelta(seconds=recon_df_w_time.loc[row, 'PredictedCumTimeInterval'])\n",
    "    return temp\n",
    "\n",
    "def fixTime(recon_df_w_time):\n",
    "    groupByCase = recon_df_w_time.groupby(['CaseID'])\n",
    "    temp = pd.DataFrame(columns=list(recon_df_w_time))\n",
    "\n",
    "    for caseid, data_case in groupByCase:\n",
    "        for row in range(1, len(data_case)):\n",
    "            current = data_case.iloc[row, data_case.columns.get_loc('PredictedTime')]\n",
    "            previous = data_case.iloc[row-1, data_case.columns.get_loc('PredictedTime')]\n",
    "            if current < previous:\n",
    "                data_case.iloc[row, data_case.columns.get_loc('PredictedTime')] = previous\n",
    "                data_case.iloc[row, data_case.columns.get_loc('PredictedCompleteTimestamp')] = data_case.iloc[row-1, data_case.columns.get_loc('PredictedCompleteTimestamp')]\n",
    "        temp = temp.append(data_case)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def evaluation(submission_df, nan_time_index, nan_activity_index, show=False):\n",
    "    #eval Time\n",
    "    true_time = submission_df.loc[nan_time_index, 'TrueTime']\n",
    "    predicted_time = submission_df.loc[nan_time_index, 'PredictedTime']\n",
    "    mae_time = mean_absolute_error(true_time, predicted_time)\n",
    "    rmse_time = sqrt(mean_squared_error(true_time, predicted_time))\n",
    "    \n",
    "    #eval Activity\n",
    "    true_activity = submission_df.loc[nan_activity_index, 'TrueActivity']\n",
    "    predicted_activity = submission_df.loc[nan_activity_index, 'PredictedActivity']\n",
    "    acc = accuracy_score(true_activity, predicted_activity)\n",
    "    \n",
    "    if show==True: \n",
    "        print('Number of missing Time: {}'.format(len(nan_time_index)))\n",
    "        print('Mean Absolute Error: {:.4f} day(s)'.format(mae_time/86400))\n",
    "        print('Root Mean Squared Error: {:.4f} day(s)'.format(rmse_time/86400))\n",
    "        \n",
    "        print('Number of missing Activity: {}'.format(len(nan_activity_index)))\n",
    "        print('Accuracy: {:.2f}%'.format(acc*100))\n",
    "    return mae_time, rmse_time, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher [-h] [-n <data_name>] [-d DATA_DIR]\n                          [--nan_pct NAN_PCT] [--train_pct TRAIN_PCT]\n                          [--val_pct VAL_PCT]\nipykernel_launcher: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9023 --control=9021 --hb=9020 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"30e3a4c5-928f-42a9-902d-e2385daff0ed\" --shell=9022 --transport=\"tcp\" --iopub=9024 --f=/tmp/tmp-15580dhmTEGCjUQsl.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None #to run loop quicker without warnings\n",
    "\n",
    "args_parser = argparse.ArgumentParser(description='induce missing data')\n",
    "args_parser.add_argument('-n', '--name', default=\"small_log\", metavar='<data_name>', help='Name of input file')\n",
    "args_parser.add_argument('-d', '--data_dir', default='../data/', help='data dir')\n",
    "args_parser.add_argument('--nan_pct', default=0.3, type=float, help='Nan percentage')\n",
    "args_parser.add_argument('--train_pct', default=0.6, type=float, help='Train percentage')\n",
    "args_parser.add_argument('--val_pct', default=0.2, type=float, help='Validate percentage')\n",
    "args = args_parser.parse_args()\n",
    "args.data_file = args.name + '.csv'\n",
    "args.input_dir = '../input/small_log/'\n",
    "\n",
    "\n",
    "#name = 'bpi_2012'\n",
    "#name = 'bpi_2013'\n",
    "#name = 'Road_Traffic_Fine_Management_Process'\n",
    "'''\n",
    "args = {\n",
    "    'data_dir': '../data/',\n",
    "    'data_file': name + '.csv',\n",
    "    'input_dir': '../input/{}/'.format(name),  \n",
    "    'nan_pct': 0.3,\n",
    "    'train_pct': 0.6,\n",
    "    'val_pct': 0.2,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)\n",
    "'''\n",
    "\n",
    "file_name = os.path.join(args.input_dir, 'parameters_{}.pkl'.format(args.nan_pct))\n",
    "with open(file_name, 'rb') as f:\n",
    "    most_frequent_activity = pickle.load(f)\n",
    "    first_timestamp = pickle.load(f)\n",
    "    avai_instance = pickle.load(f)\n",
    "    nan_instance = pickle.load(f)\n",
    "    train_size = pickle.load(f)\n",
    "    val_size = pickle.load(f)\n",
    "    test_size = pickle.load(f)\n",
    "    train_row_num = pickle.load(f)\n",
    "    val_row_num = pickle.load(f)\n",
    "    test_row_num = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7ac61d06ed5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parameters_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmost_frequent_activity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfirst_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = os.path.join(args.input_dir, 'parameters_{}.pkl'.format(args.nan_pct))\n",
    "with open(file_name, 'rb') as f:\n",
    "    most_frequent_activity = pickle.load(f)\n",
    "    first_timestamp = pickle.load(f)\n",
    "    avai_instance = pickle.load(f)\n",
    "    nan_instance = pickle.load(f)\n",
    "    train_size = pickle.load(f)\n",
    "    val_size = pickle.load(f)\n",
    "    test_size = pickle.load(f)\n",
    "    train_row_num = pickle.load(f)\n",
    "    val_row_num = pickle.load(f)\n",
    "    test_row_num = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1dc6e491beaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcomplete_df_full_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'complete_df_full_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmissing_df_full_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'missing_df_full_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load data\n",
    "complete_df_full_name = 'complete_df_full_{}.csv'.format(args.nan_pct)\n",
    "missing_df_full_name = 'missing_df_full_{}.csv'.format(args.nan_pct)\n",
    "print('Loading data:')\n",
    "print(args.name)\n",
    "print(complete_df_full_name)\n",
    "print(missing_df_full_name)\n",
    "\n",
    "df_name = os.path.join(args.input_dir, complete_df_full_name)\n",
    "df = pd.read_csv(df_name)\n",
    "\n",
    "missing_df_name = os.path.join(args.input_dir, missing_df_full_name)\n",
    "missing_df = pd.read_csv(missing_df_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing data...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12f456dad8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgroupByCase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CaseID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroupByCase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CaseID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "print('Processing data...')\n",
    "groupByCase = df.groupby(['CaseID'])\n",
    "\n",
    "groupByCase = df.groupby(['CaseID'])\n",
    "missing_groupByCase = missing_df.groupby(['CaseID'])\n",
    "\n",
    "normalized_complete_df = pd.DataFrame(columns=list(df)+['NormalizedTime'])\n",
    "normalized_missing_df = pd.DataFrame(columns=list(df)+['NormalizedTime'])\n",
    "min_max_storage = {}\n",
    "\n",
    "for i, j in zip(groupByCase, missing_groupByCase):\n",
    "    temp, missing_temp, missing_case_storage = minmaxScaler(i[0], i[1], j[1])\n",
    "    normalized_complete_df = normalized_complete_df.append(temp)\n",
    "    normalized_missing_df = normalized_missing_df.append(missing_temp)\n",
    "    min_max_storage.update(missing_case_storage)\n",
    "\n",
    "\n",
    "cat_var = ['Activity']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'OHE' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-29d9ec5e3c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# OHE: get k dummies out of k categorical levels (drop_first=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menc_complete_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_complete_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menc_missing_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_missing_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OHE' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# OHE: get k dummies out of k categorical levels (drop_first=False)\n",
    "enc_complete_df = OHE(normalized_complete_df, cat_var)\n",
    "enc_missing_df = OHE(normalized_missing_df, cat_var)\n",
    "\n",
    "print('Getting masks...')\n",
    "\n",
    "c_df = enc_complete_df.copy()\n",
    "m_df = enc_missing_df.copy()\n",
    "enc_complete_df_w_normalized_time = c_df.drop(['CompleteTimestamp', 'CumTimeInterval'], axis=1)\n",
    "enc_missing_df_w_normalized_time = m_df.drop(['CompleteTimestamp', 'CumTimeInterval'], axis=1)\n",
    "\n",
    "c_df = enc_complete_df.copy()\n",
    "m_df = enc_missing_df.copy()\n",
    "enc_complete_df_w_time = c_df.drop(['CompleteTimestamp', 'NormalizedTime'], axis=1)\n",
    "enc_missing_df_w_time = m_df.drop(['CompleteTimestamp', 'NormalizedTime'], axis=1)\n",
    "\n",
    "avai_index_df = enc_missing_df_w_time.copy()\n",
    "nan_index_df = enc_missing_df_w_time.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mask for Time\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'enc_missing_df_w_time' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b8482e491ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#mask for Time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mask for Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_missing_df_w_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_missing_df_w_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CumTimeInterval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if nan Time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mavai_index_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CumTimeInterval'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_missing_df_w_time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#mask for Time\n",
    "print('Mask for Time')\n",
    "for row in range(enc_missing_df_w_time.shape[0]):\n",
    "    if np.isnan(enc_missing_df_w_time.loc[row, 'CumTimeInterval']): # if nan Time\n",
    "        avai_index_df.loc[row, 'CumTimeInterval'] = 0\n",
    "        nan_index_df.loc[row, 'CumTimeInterval'] = 1\n",
    "    else:\n",
    "        avai_index_df.loc[row, 'CumTimeInterval'] = 1\n",
    "        nan_index_df.loc[row, 'CumTimeInterval'] = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "#mask for Activity\n",
    "print('Mask for Activity')\n",
    "for row in range(enc_missing_df_w_time.shape[0]):\n",
    "    if np.any(enc_missing_df_w_time.iloc[row,2:]>0): #if avai Time\n",
    "        avai_index_df.iloc[row, 2:] = 1\n",
    "        nan_index_df.iloc[row, 2:] = 0\n",
    "    else:\n",
    "        avai_index_df.iloc[row, 2:] = 0\n",
    "        nan_index_df.iloc[row, 2:] = 1\n",
    "        \n",
    "pad_index_df = enc_complete_df.copy()\n",
    "cols = [x for x in list(pad_index_df) if x != 'CaseID']\n",
    "pad_index_df.loc[:, cols] = 1\n",
    "\n",
    "enc_missing_df_w_normalized_time.fillna(0, inplace=True)\n",
    "enc_missing_df_w_time.fillna(0, inplace=True)\n",
    "\n",
    "enc_complete_w_normalized_time_groupByCase = enc_complete_df_w_normalized_time.groupby(['CaseID'])\n",
    "enc_missing_w_normalized_time_groupByCase = enc_missing_df_w_normalized_time.groupby(['CaseID'])\n",
    "\n",
    "enc_complete_w_time_groupByCase = enc_complete_df_w_time.groupby(['CaseID'])\n",
    "enc_missing_w_time_groupByCase = enc_missing_df_w_time.groupby(['CaseID'])\n",
    "\n",
    "avai_index_df_groupByCase = avai_index_df.groupby(['CaseID'])\n",
    "nan_index_df_groupByCase = nan_index_df.groupby(['CaseID'])\n",
    "pad_index_df_groupByCase = pad_index_df.groupby(['CaseID'])\n",
    "\n",
    "maxlen = findLongestLength(groupByCase)\n",
    "print('Length of longest case: {}'.format(maxlen))\n",
    "\n",
    "cols_w_time = [i for i in list(enc_complete_df_w_time) if i != 'CaseID']\n",
    "cols_w_normalized_time = [i for i in list(enc_complete_df_w_normalized_time) if i != 'CaseID']\n",
    "\n",
    "vectorized_complete_df_w_normalized_time = getInput(enc_complete_w_normalized_time_groupByCase, cols_w_normalized_time, maxlen)\n",
    "vectorized_missing_df_w_normalized_time = getInput(enc_missing_w_normalized_time_groupByCase, cols_w_normalized_time, maxlen)\n",
    "\n",
    "vectorized_complete_df_w_time = getInput(enc_complete_w_time_groupByCase, cols_w_time, maxlen)\n",
    "vectorized_missing_df_w_time = getInput(enc_missing_w_time_groupByCase, cols_w_time, maxlen)\n",
    "\n",
    "vectorized_avai_index_df = getInput(avai_index_df_groupByCase, cols_w_time, maxlen)\n",
    "vectorized_nan_index_df = getInput(nan_index_df_groupByCase, cols_w_time, maxlen)\n",
    "vectorized_pad_index_df = getInput(pad_index_df_groupByCase, cols_w_time, maxlen)\n",
    "\n",
    "\n",
    "complete_matrix_w_normalized_time = vectorized_complete_df_w_normalized_time\n",
    "missing_matrix_w_normalized_time = vectorized_missing_df_w_normalized_time\n",
    "\n",
    "avai_matrix = vectorized_avai_index_df\n",
    "nan_matrix = vectorized_nan_index_df\n",
    "pad_matrix = vectorized_pad_index_df\n",
    "\n",
    "\n",
    "print('Saving preprocessed data...')\n",
    "preprocessed_data_name = os.path.join(args.input_dir, 'preprocessed_data_full_{}.pkl'.format(args.nan_pct))\n",
    "with open(preprocessed_data_name, 'wb') as f:\n",
    "    pickle.dump(min_max_storage, f, protocol=2)\n",
    "    pickle.dump(complete_matrix_w_normalized_time, f, protocol=2)\n",
    "    pickle.dump(missing_matrix_w_normalized_time, f, protocol=2)\n",
    "    pickle.dump(avai_matrix, f, protocol=2)\n",
    "    pickle.dump(nan_matrix, f, protocol=2)\n",
    "    pickle.dump(pad_matrix, f, protocol=2)\n",
    "    pickle.dump(cols_w_time, f, protocol=2)\n",
    "    pickle.dump(cols_w_normalized_time, f, protocol=2)\n",
    "    \n",
    "print('Finish!!!')"
   ]
  }
 ]
}